# Gateway

Project scaffolding for a lightweight LLM gateway service. Uses Poetry for dependency management and includes recommended dev tooling (Black, Ruff, MyPy, Pytest, and pre-commit) plus a FastAPI-based controller that can proxy OpenAI, Gemini, Claude, or the built-in echo provider.

## Requirements

- Python 3.11 (recommended via `pyenv`)
- [Poetry 2.x](https://python-poetry.org/docs/#installation)

## Installation

```bash
poetry install
# or the stricter resolver: poetry sync
```

This creates a virtual environment (see `poetry env info`) and installs the pinned dependencies from `poetry.lock`.

Copy `.env.example` to `.env` and populate the provider keys you intend to use:

```bash
cp .env.example .env
# edit the file as needed
```

## Development workflow

| Task        | Command                      | Notes                              |
|-------------|------------------------------|------------------------------------|
| Run tests   | `poetry run pytest` or `make test` | Minimal Pytest config lives in `pyproject.toml`. |
| Lint        | `poetry run ruff check src tests` or `make lint` | Enforces Ruff lint rules. |
| Format      | `make format`                | Runs Black then auto-fixes via Ruff. |
| Type-check  | `make type-check`            | Strict MyPy config checks `src/`. |
| All hooks   | `make pre-commit`            | Runs the configured pre-commit hooks. |

To keep commits clean:

```bash
poetry run pre-commit install
```

## Running the gateway

Use the Makefile as the single entry point:

```bash
# local dev (reload)
make serve

# containerized
make docker-build
make start      # brings the gateway up via docker compose
make restart    # down + up
make stop       # tear down containers
```

Key endpoints:

- `GET /healthz` – liveness plus provider list.
- `GET /readyz` – readiness check that verifies OpenAI key presence and basic reachability.
- `POST /v1/responses` – Accepts native OpenAI Responses payloads (`model`, `input`, `response_format`, etc.) and streams spec-compliant SSE events.
- `POST /v1/agents/messages` & `GET /v1/agents/{agent_id}/messages` – lightweight in-memory bus for inter-agent messaging/hand-offs.

Attach `X-Request-ID` headers to correlate client requests with gateway traces.

### Provider configuration

- Set `OPENAI_KEY`, `GEMINI_KEY`, `CLAUDE_KEY`, etc. in `.env`.
- Choose a default provider by prefixing the model (e.g. `"openai:gpt-5-nano"`, `"gemini:gemini-2.5-pro-preview-03-25"`, or `"echo:test-model"`). No provider is assumed if you omit the prefix.
- Prefix the `model` field with the provider when calling `/v1/responses`; if you omit it and no `DEFAULT_PROVIDER` is set in the environment, the request will fail with `provider_required`.
- The OpenAI adapter targets the Responses API (`/v1/responses`) so advanced models like `gpt-5-nano` can use reasoning controls by sending `reasoning_effort`. The Gemini adapter defaults to `gemini-2.5-pro-preview-03-25` on the public API unless you override it by passing a different model name.
- The echo provider is always available for local testing and CI.

### Streaming contract

- OpenAI requests are streamed through verbatim from the Responses API (SSE), including upstream event names and request IDs.
- Other providers emit a single `response.created` → `response.output_text.delta` → `response.completed` sequence generated by the gateway.
- Keep the connection open through completion; use `--no-buffer` or equivalent on clients to avoid buffering.

### Smoke tests

Use the helper script to call each provider with real credentials (make sure `.env` is populated first):

```bash
poetry run python scripts/smoke_tests.py
```

The script prints a one-line summary per provider/model. Failures (e.g., missing quota) are surfaced inline so you can distinguish gateway bugs from upstream issues quickly.

### Tracing & retries

The gateway seeds a trace ID per request (exposed as `trace_id` in responses and `X-Request-ID` headers) and uses a shared `httpx.AsyncClient` so you can easily add retries, circuit breakers, or observability exporters later.
- `make docker-build` / `make docker-up` – build/run the container via `docker compose` using your `.env`.

### Agent helper

If you're wiring multiple agents to the gateway, use the included `GatewayAgentClient` to avoid rewriting curl snippets. It keeps an `httpx.AsyncClient` alive, parses the Responses SSE stream, and can bundle local images as multimodal input:

```python
import asyncio
from gateway.client import GatewayAgentClient, build_user_message


async def main() -> None:
    input_messages = [
        {"role": "system", "content": "You are a meticulous captioning agent."},
        build_user_message(
            "Describe this chart in one sentence.",
            image_paths=["/path/to/chart.png"],
        ),
    ]

    async with GatewayAgentClient() as client:
        result = await client.complete_response(
            model="openai:gpt-5-nano",
            input_messages=input_messages,
        )

    print(result["text"])


if __name__ == "__main__":
    asyncio.run(main())
```

Call `client.stream_response(...)` if you want the raw Responses SSE events; `client.complete_response(...)` buffers the text for you. Pass `response_format={"type": "json_object"}` or `reasoning={"effort": "high"}` to forward advanced controls directly to OpenAI.

Need a blocking/synchronous call? Use `complete_response_sync` which wraps the async client internally:

```python
from gateway.client import complete_response_sync

text = complete_response_sync(
    model="echo:test-model",
    prompt="Hello from a sync call",
    base_url="http://localhost:8000",  # defaults to env GATEWAY_URL or 127.0.0.1:8000
)
print(text)
```

## Optional EDGAR pipeline gateway

The `gateway.edgar` module retains the EDGAR-specific `/jobs` workflow that fetches
segments from tarballs, normalizes them, and builds prompts before calling OpenAI. It lives
alongside the provider-agnostic API so repositories like
`credit-agreement-extraction` can reuse the same code via a submodule. The module expects
`edgar_filing_pipeline` to be importable (the credit project installs it), so avoid invoking
`gateway.edgar.cli` in environments that lack those dependencies.
